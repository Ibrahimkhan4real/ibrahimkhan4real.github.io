---
layout: default
title: "AI Demos â€” Muhammad Ibrahim Khan"
---

<header>
  <p>Interactive Reinforcement Learning</p>
</header>

<article>
  <div id="article_title">
    <h1>Interactive AI Demos</h1>
  </div>
  <div id="article_text">
    <p class="lead">Research is better when you can see it in action. Below are browser-based visualizations of the algorithms I use in my PhD.</p>

    <section class="status-section">
      <h2>MCTS: Decision Tree Visualization</h2>
      <p>This demo visualizes how <strong>Monte Carlo Tree Search</strong> explores potential actions for a domestic hot-water tank. It builds a tree by simulating future states to find the most energy-efficient heating schedule.</p>

      <div id="mcts-container" style="width: 100%; height: 400px; border: 1px solid var(--border); border-radius: 8px; background: var(--panel); position: relative; overflow: hidden;">
        <canvas id="mcts-canvas"></canvas>
        <div id="mcts-controls" style="position: absolute; bottom: 10px; left: 10px; display: flex; gap: 10px;">
          <button class="button_accent" id="mcts-step">Step Exploration</button>
          <button class="button_accent" id="mcts-reset">Reset</button>
        </div>
      </div>
      <p class="travel-note">Nodes represent states (Time, Temp). Edges show actions (Heat ON/OFF). Color intensity represents the Reward (Energy Saved + Comfort).</p>
    </section>

    <section class="status-section">
      <h2>Q-Learning Grid World</h2>
      <p>Watch a <strong>Q-learning</strong> agent learn the optimal policy in an 8&times;8 grid. Click cells to toggle walls, shift-click or right-click to set the goal. Arrow direction shows the best action; color intensity shows Q-value magnitude.</p>

      <div id="ql-container" class="demo-container">
        <canvas id="ql-canvas"></canvas>
        <div class="demo-controls">
          <button class="button_accent" id="ql-train1">Train 1 Episode</button>
          <button class="button_accent" id="ql-train100">Train 100 Episodes</button>
          <button class="button_accent" id="ql-reset">Reset</button>
        </div>
        <div class="demo-stats" id="ql-stats"></div>
      </div>
      <p class="travel-note">S = start (top-left), G = goal (bottom-right by default). Grey cells are walls. The agent uses &epsilon;-greedy exploration with decay (&alpha;=0.1, &gamma;=0.95).</p>
    </section>

    <section class="status-section">
      <h2>MCTS vs Greedy Comparison</h2>
      <p>Compare <strong>Monte Carlo Tree Search</strong> (UCB1 selection + random rollouts) against a <strong>Greedy</strong> strategy on the same random decision tree. MCTS explores broadly while Greedy commits early.</p>

      <div class="demo-side-by-side">
        <div class="demo-panel" id="mvg-mcts-panel">
          <div class="demo-panel-title">MCTS (UCB1)</div>
          <canvas id="mvg-mcts-canvas"></canvas>
        </div>
        <div class="demo-panel" id="mvg-greedy-panel">
          <div class="demo-panel-title">Greedy</div>
          <canvas id="mvg-greedy-canvas"></canvas>
        </div>
      </div>
      <div class="demo-container" style="border-top: none; border-radius: 0 0 8px 8px; margin-top: -1px;">
        <div class="demo-controls">
          <button class="button_accent" id="mvg-generate">New Tree</button>
          <button class="button_accent" id="mvg-run">Run Comparison</button>
          <button class="button_accent" id="mvg-step">Step</button>
          <label class="demo-slider-label">Speed
            <input type="range" class="demo-slider" id="mvg-speed" min="1" max="100" value="50">
          </label>
        </div>
        <div class="demo-stats" id="mvg-stats"></div>
      </div>
      <p class="travel-note">Leaf nodes show reward values. Node numbers show visit counts (MCTS). Highlighted paths show the final chosen route. MCTS uses C=&radic;2 for exploration.</p>
    </section>

    <section class="status-section">
      <h2>Multi-Armed Bandit Playground</h2>
      <p>Explore the <strong>exploration vs exploitation</strong> dilemma. Each arm is a slot machine with a hidden reward probability. Click an arm to pull it manually, or use an automated strategy.</p>

      <div id="bandit-container" class="demo-container">
        <canvas id="bandit-canvas"></canvas>
        <canvas id="bandit-regret-canvas"></canvas>
        <div class="demo-controls">
          <select class="demo-select" id="bandit-strategy">
            <option value="epsilon">&epsilon;-Greedy (&epsilon;=0.1)</option>
            <option value="ucb1">UCB1</option>
          </select>
          <button class="button_accent" id="bandit-auto">Auto Run 100</button>
          <button class="button_accent" id="bandit-reset">Reset</button>
        </div>
        <div class="demo-stats" id="bandit-stats"></div>
      </div>
      <p class="travel-note">Click directly on an arm bar to pull it manually. The bar chart shows estimated reward per arm; the line chart tracks cumulative regret over time. Each arm is a Bernoulli bandit with fixed hidden probability.</p>
    </section>
  </div>
</article>

<script src="{{ "/assets/js/mcts-demo.js" | relative_url }}"></script>
<script src="{{ "/assets/js/qlearning-demo.js" | relative_url }}"></script>
<script src="{{ "/assets/js/mcts-vs-greedy-demo.js" | relative_url }}"></script>
<script src="{{ "/assets/js/bandit-demo.js" | relative_url }}"></script>
